{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Œ í˜•íƒœì†Œ ë¶„ì„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœï¸ íŒŒì¼ ì½ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# íŒŒì¼ ì½ê¸°\n",
    "def read_txt_file(file_name):\n",
    "  print(\"[íŒŒì¼ëª…] \" + file_name)\n",
    "  text = \"\"\n",
    "  \n",
    "  # íŒŒì¼ ì—´ê¸°\n",
    "  with open(file_path + file_name, \"r\", encoding=\"UTF8\") as txt_file:\n",
    "    # íŒŒì¼ ë¼ì¸ë³„ë¡œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ\n",
    "    for line in txt_file:\n",
    "      line = line.strip()\n",
    "      text += line\n",
    "  return text\n",
    "\n",
    "file_path = \"C:\\\\Users\\\\sodud\\\\study\\\\ssafy_spec_pjt\\\\subtitle\\\\data\\\\refined_text\\\\\"\n",
    "\n",
    "# ìžë§‰ íŒŒì¼ í´ë”\n",
    "file_list = os.listdir(file_path)\n",
    "\n",
    "# ìžë§‰ íŒŒì¼ ëª©ë¡\n",
    "txt_files = [txt_file for txt_file in file_list]\n",
    "\n",
    "for txt_file in txt_files:\n",
    "  read_txt_file(txt_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœï¸ ë¶ˆìš©ì–´ ì œê±°\n",
    "* ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ ì§ì ‘ ì„¤ì •  \n",
    "* nltkì— íŒŒì¼ ìƒì„± í›„ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© (âœ”ï¸)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
    "stop_words = []\n",
    "with open(\"stop_words_japanese.txt\", \"r\", encoding=\"UTF8\") as f :\n",
    "  lines = f.readlines()\n",
    "  # for stop_word in lines:\n",
    "  #   stop_words.append(stop_word.strip())\n",
    "  stop_words = [stop_word.strip() for stop_word in lines]\n",
    "\n",
    "# ìƒì„±ëœ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ í™•ì¸\n",
    "print(\"ë¶ˆìš©ì–´: \", stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœï¸ ì‚¬ì „ ì •ë³´ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename: C:\\Users\\sodud\\AppData\\Roaming\\Python\\Python39\\site-packages\\unidic_lite\\dicdir\\sys.dic\n",
      "charset: utf8\n",
      "size: 756264\n",
      "type: 0\n",
      "lsize: 5981\n",
      "rsize: 5981\n",
      "version: 102\n"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "\n",
    "try:\n",
    "  # í’ˆì‚¬ ë¶„ì„\n",
    "  t = MeCab.Tagger()\n",
    "  \n",
    "  # ì¼ë³¸ì–´ ì‚¬ì „ ì •ë³´ ì¶œë ¥\n",
    "  d = t.dictionary_info()\n",
    "  while d:\n",
    "    print(\"filename: %s\" % d.filename)\n",
    "    print(\"charset: %s\" %  d.charset)\n",
    "    print(\"size: %d\" %  d.size)\n",
    "    print(\"type: %d\" %  d.type)\n",
    "    print(\"lsize: %d\" %  d.lsize)\n",
    "    print(\"rsize: %d\" %  d.rsize)\n",
    "    print(\"version: %d\" %  d.version)\n",
    "    d = d.next\n",
    "\n",
    "\n",
    "except RuntimeError as e:\n",
    "  print(\"RuntimeError:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import MeCab\n",
    "import neologdn\n",
    "from nltk.probability import FreqDist\n",
    "from matplotlib import rcParams\n",
    "\n",
    "rcParams[\"font.family\"] = 'sans-serif'\n",
    "  \n",
    "# í¬í•¨ í’ˆì‚¬: ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬, ë¶€ì‚¬\n",
    "CONTENT_WORD_POS = (\"åè©ž\", \"å‹•è©ž\", \"å½¢å®¹è©ž\", \"å‰¯è©ž\")\n",
    "# ì œì™¸ í’ˆì‚¬: ì ‘ë¯¸ì‚¬, ë¹„ìžë¦½, ëŒ€ëª…ì‚¬, ë¯¸ì§€ì–´\n",
    "IGNORE = (\"æŽ¥å°¾\", \"éžè‡ªç«‹\", \"ä»£åè©ž\", \"æœªçŸ¥èªž\")\n",
    "\n",
    "def is_content_word(feature):\n",
    "  return feature.startswith(CONTENT_WORD_POS) and all(f not in IGNORE for f in feature.split(\",\")[:7])\n",
    "\n",
    "sentence = \"\"\n",
    "file_path = \"C:\\\\Users\\\\sodud\\\\study\\\\ssafy_spec_pjt\\\\subtitle\\\\data\\\\refined_text\\\\\"\n",
    "text = open(file_path + \"Getter_Robot_Go_001.txt\", \"r\", encoding=\"UTF8\")\n",
    "for line in text:\n",
    "  sentence += line\n",
    "\n",
    "try:\n",
    "  # í’ˆì‚¬ ë¶„ì„\n",
    "  tagger = MeCab.Tagger()\n",
    "  \n",
    "  # ê¸°ë³¸ ì¶œë ¥ ê²°ê³¼\n",
    "  # print(tagger.parse(sentence))\n",
    "  \n",
    "  sentence = neologdn.normalize(sentence,repeat = 2)\n",
    "  sentence = \"\".join([i for i in sentence if i.isalpha() or i.isspace()])\n",
    "  result = tagger.parseToNode(sentence)\n",
    "  \n",
    "  stop_words = []\n",
    "  with open(\"stop_words_japanese.txt\", \"r\", encoding=\"UTF8\") as f :\n",
    "    lines = f.readlines()\n",
    "    stop_words = [stop_word.strip() for stop_word in lines]\n",
    "    \n",
    "  content_words = []\n",
    "  \n",
    "  while result:\n",
    "    if is_content_word(result.feature):\n",
    "      lemma = result.feature.split(\",\")[7] if len(result.feature.split(\",\")) > 7 and result.feature.split(\",\")[7] != \"*\" else result.surface\n",
    "      if lemma not in stop_words:\n",
    "        content_words.append(lemma.split(\"-\")[0])\n",
    "    result = result.next\n",
    "  \n",
    "  print(content_words)\n",
    "  \n",
    "  fdist = FreqDist(content_words)\n",
    "  fdist.plot(10, cumulative=False)\n",
    "  \n",
    "except RuntimeError as e:\n",
    "    print(\"RuntimeError:\", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eebaf1173d8d9c3c4ee9a7b8bb1432a7f576348d6cb7a26bc263375fbc310797"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
