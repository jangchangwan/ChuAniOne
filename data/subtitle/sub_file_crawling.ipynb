{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“Œ ìë§‰ ë°ì´í„° í¬ë¡¤ë§  \n",
    "* urllib ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©í•´ íŒŒì¼ ë‹¤ìš´ë¡œë“œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœï¸ ì• ë‹ˆë©”ì´ì…˜ ìë§‰ í˜ì´ì§€ ëª©ë¡ ë°›ì•„ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì½ì–´ë“¤ì´ê¸°\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "# íŒŒì¼ ì£¼ì†Œ í¬ë¡¤ë§\n",
    "def bs_parsing(url):\n",
    "  response = requests.get(root_url + url)\n",
    "  soup = bs(response.text, \"html.parser\")\n",
    "  elements = soup.select(\"table#flisttable td > a\")\n",
    "  \n",
    "  # ì£¼ì†Œ ë‹´ì„ ë¦¬ìŠ¤íŠ¸\n",
    "  page_list = []\n",
    "  \n",
    "  # ìƒì„¸ í˜ì´ì§€ ì£¼ì†Œ ë¦¬ìŠ¤íŠ¸í™”\n",
    "  for (idx, el) in enumerate(elements, 1):\n",
    "    page_list.append(root_url + el.attrs[\"href\"].replace(\"%2F\", \"/\"))\n",
    "    \n",
    "  return page_list\n",
    "\n",
    "# ë¦¬ìŠ¤íŠ¸ ê°’ì„ txt íŒŒì¼ì— í•œ ì¤„ì”© ì €ì¥\n",
    "def save_text_file(page_list):\n",
    "  f = open(\"C:/Users/sodud/study/ssafy_spec_pjt/subtitle/data/page_url_list.txt\", \"w\", encoding=\"UTF8\")\n",
    "  for page in page_list:\n",
    "    f.write(page)\n",
    "    f.write(\"\\n\")\n",
    "  f.close\n",
    "  (\"[SUCCESS] ìë§‰ í˜ì´ì§€ ëª©ë¡ ì €ì¥\")\n",
    "\n",
    "# ìë§‰ ì‚¬ì´íŠ¸\n",
    "root_url = \"https://kitsunekko.net\"\n",
    "\n",
    "# ì¼ë³¸ì–´ ìë§‰ ëª©ë¡ í˜ì´ì§€\n",
    "page_url = \"/dirlist.php?dir=subtitles/japanese/\"\n",
    "\n",
    "# ìë§‰ í˜ì´ì§€ ëª©ë¡ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥\n",
    "page_list = bs_parsing(page_url)\n",
    "\n",
    "# ë¦¬ìŠ¤íŠ¸ë¥¼ txt íŒŒì¼ë¡œ ì €ì¥\n",
    "save_text_file(page_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœï¸ srt í™•ì¥ì íŒŒì¼ë§Œ ë‹¤ìš´ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import os\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from inspect import getfile\n",
    "from urllib import request\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "def read_text_file(page_list_file, start, unit):\n",
    "  flag = True\n",
    "  f = open(page_list_file, \"r\", encoding=\"UTF8\")\n",
    "  lines = f.readlines()[start: start+unit]\n",
    "  \n",
    "  if len(lines) == 0:\n",
    "    return\n",
    "  \n",
    "  if len(lines) < unit:\n",
    "    flag = False\n",
    "  \n",
    "  for line in lines:\n",
    "    \n",
    "    # ì£¼ì†Œê°€ ì•„ë‹ˆë©´ pass\n",
    "    if line[-3:] != \"%2F\":\n",
    "      pass\n",
    "    \n",
    "    response = requests.get(line[:-1])\n",
    "    soup = bs(response.text, \"html.parser\")\n",
    "    files = soup.select(\"table#flisttable td > a\")\n",
    "    \n",
    "    # íŒŒì¼ ì£¼ì†Œ ë‹´ì„ ë¦¬ìŠ¤íŠ¸\n",
    "    file_urls = []\n",
    "    \n",
    "    # íŒŒì¼ ì½ê¸°\n",
    "    for f in files:\n",
    "      file_path= f.attrs[\"href\"]\n",
    "      \n",
    "      # srt íŒŒì¼ë§Œ ì €ì¥\n",
    "      if file_path[-3:] == \"srt\":\n",
    "        file_urls.append(\"/\" + file_path)\n",
    "      else:\n",
    "        pass\n",
    "    \n",
    "    # ìë§‰ í˜ì´ì§€ í™•ì¸\n",
    "    print(file_path)\n",
    "    \n",
    "    # íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ ê²€ìƒ‰\n",
    "    file_search(file_urls)\n",
    "    \n",
    "  f.close\n",
    "  \n",
    "  if flag == False:\n",
    "    return\n",
    "  \n",
    "  return read_text_file(page_list_file, start + unit, unit)\n",
    "\n",
    "# íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ ê²€ìƒ‰\n",
    "def file_search(file_urls):\n",
    "  for file_url in file_urls:\n",
    "    # íŒŒì¼ ì´ë¦„ë§Œ ì¶”ì¶œ\n",
    "    depth = file_url.split(\"/\")\n",
    "    filename = depth[len(depth)-1]\n",
    "    try:\n",
    "      # íŒŒì¼ ì €ì¥ ê²½ë¡œ\n",
    "      path = \"C:\\\\Users\\\\sodud\\\\study\\\\ssafy_spec_pjt\\\\subtitle\\\\data\\\\text\\\\\"\n",
    "      \n",
    "      # íŒŒì¼ì´ ì €ì¥ë˜ì–´ ìˆì§€ ì•Šìœ¼ë©´ ë‹¤ìš´ë¡œë“œ\n",
    "      if os.path.isfile(path + filename):\n",
    "        print(\"ì´ë¯¸ ì¡´ì¬í•˜ëŠ” íŒŒì¼: \" + filename)\n",
    "      else:\n",
    "        get_download(root_url + file_url.replace(\" \", \"%20\"), filename, path)\n",
    "        \n",
    "    except HTTPError as err:\n",
    "      print(\"ë‹¤ìš´ë¡œë“œ ì¤‘ ì—ëŸ¬ ë°œìƒ: \" + filename + \"/\" + err)\n",
    "      pass\n",
    "\n",
    "# íŒŒì¼ ë‹¤ìš´ë¡œë“œ\n",
    "\"\"\"\n",
    "[*]   url: íŒŒì¼ ë‹¤ìš´ë¡œë“œ ê²½ë¡œ\n",
    "[*] fname: ì €ì¥í•  íŒŒì¼ëª…\n",
    "[*]   dir: ì €ì¥ í´ë” ê²½ë¡œ\n",
    "\"\"\"\n",
    "def get_download(url, fname, dir):\n",
    "  try:\n",
    "    os.chdir(dir)\n",
    "    # ë‹¤ìš´ë¡œë“œ ìš”ì²­\n",
    "    request.urlretrieve(url, fname)\n",
    "    print(\"ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: \" + fname)\n",
    "  except HTTPError as err:\n",
    "    print(\"ì—ëŸ¬ ë°œìƒ:\" + err)\n",
    "    return\n",
    "\n",
    "# ìë§‰ ì‚¬ì´íŠ¸\n",
    "root_url = \"https://kitsunekko.net\"\n",
    "\n",
    "# ìë§‰ íŒŒì¼ í˜ì´ì§€ ëª©ë¡ íŒŒì¼\n",
    "page_list_file = \"C:/Users/sodud/study/ssafy_spec_pjt/subtitle/data/page_url_list.txt\"\n",
    "\n",
    "# ëª©ë¡ íŒŒì¼ ì½ê¸°\n",
    "read_text_file(page_list_file, 0, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
