{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“Œ ìë§‰ ë°ì´í„° í¬ë¡¤ë§  \n",
    "* urllib ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©í•´ íŒŒì¼ ë‹¤ìš´ë¡œë“œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœï¸ ì• ë‹ˆë©”ì´ì…˜ ìë§‰ í˜ì´ì§€ ëª©ë¡ ë°›ì•„ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì½ì–´ë“¤ì´ê¸°\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "# íŒŒì¼ ì£¼ì†Œ í¬ë¡¤ë§\n",
    "def bs_parsing(url):\n",
    "  page_url = url\n",
    "  response = requests.get(root_url + page_url)\n",
    "  \n",
    "  soup = bs(response.text, \"html.parser\")\n",
    "  elements = soup.select(\"table#flisttable td > a\")\n",
    "    \n",
    "  page_list = []\n",
    "  \n",
    "  # ìƒì„¸ í˜ì´ì§€ ì£¼ì†Œ ë¦¬ìŠ¤íŠ¸í™”\n",
    "  for (idx, el) in enumerate(elements, 1):\n",
    "    page_list.append(root_url + el.attrs[\"href\"].replace(\"%2F\", \"/\"))\n",
    "    \n",
    "  return page_list\n",
    "\n",
    "def div_list(page_list):\n",
    "  index_list = [\"spec\"]\n",
    "  for al in range(65, 91):\n",
    "    index_list.append(chr(al))\n",
    "\n",
    "def save_text_file(page_list):\n",
    "  f = open(\"C:/Users/sodud/study/ssafy_spec_pjt/subtitle/page_url_list.txt\", \"w\", encoding=\"UTF8\")\n",
    "  for page in page_list:\n",
    "    f.write(page)\n",
    "    f.write(\"\\n\")\n",
    "  f.close\n",
    "\n",
    "# ìë§‰ ì‚¬ì´íŠ¸\n",
    "root_url = \"https://kitsunekko.net\"\n",
    "\n",
    "# ì¼ë³¸ì–´ ìë§‰ ëª©ë¡ í˜ì´ì§€\n",
    "page_url = \"/dirlist.php?dir=subtitles/japanese/\"\n",
    "\n",
    "page_list = bs_parsing(page_url)\n",
    "save_text_file(page_list)\n",
    "print(page_list)\n",
    "# index_list = div_list(page_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subtitles/japanese/Darwin's Game/[HorribleSubs] Darwin's Game S1 [1080p].zip\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"HTTPError\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\sodud\\study\\ssafy_spec_pjt\\subtitle\\sub_file_crawling.ipynb ì…€ 5\u001b[0m in \u001b[0;36mget_download\u001b[1;34m(url, fname, dir)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sodud/study/ssafy_spec_pjt/subtitle/sub_file_crawling.ipynb#W5sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m os\u001b[39m.\u001b[39mchdir(\u001b[39mdir\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/sodud/study/ssafy_spec_pjt/subtitle/sub_file_crawling.ipynb#W5sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m request\u001b[39m.\u001b[39;49murlretrieve(url, fname)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sodud/study/ssafy_spec_pjt/subtitle/sub_file_crawling.ipynb#W5sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39më‹¤ìš´ë¡œë“œ ì™„ë£Œ: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m fname)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py:239\u001b[0m, in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m    237\u001b[0m url_type, path \u001b[39m=\u001b[39m _splittype(url)\n\u001b[1;32m--> 239\u001b[0m \u001b[39mwith\u001b[39;00m contextlib\u001b[39m.\u001b[39mclosing(urlopen(url, data)) \u001b[39mas\u001b[39;00m fp:\n\u001b[0;32m    240\u001b[0m     headers \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39minfo()\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py:214\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    213\u001b[0m     opener \u001b[39m=\u001b[39m _opener\n\u001b[1;32m--> 214\u001b[0m \u001b[39mreturn\u001b[39;00m opener\u001b[39m.\u001b[39;49mopen(url, data, timeout)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py:523\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    522\u001b[0m     meth \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 523\u001b[0m     response \u001b[39m=\u001b[39m meth(req, response)\n\u001b[0;32m    525\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py:632\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m code \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m):\n\u001b[1;32m--> 632\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparent\u001b[39m.\u001b[39;49merror(\n\u001b[0;32m    633\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39mhttp\u001b[39;49m\u001b[39m'\u001b[39;49m, request, response, code, msg, hdrs)\n\u001b[0;32m    635\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py:561\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    560\u001b[0m args \u001b[39m=\u001b[39m (\u001b[39mdict\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhttp_error_default\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m+\u001b[39m orig_args\n\u001b[1;32m--> 561\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_chain(\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py:494\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    493\u001b[0m func \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 494\u001b[0m result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    495\u001b[0m \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py:641\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    640\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhttp_error_default\u001b[39m(\u001b[39mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[1;32m--> 641\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(req\u001b[39m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 404: Not Found",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\sodud\\study\\ssafy_spec_pjt\\subtitle\\sub_file_crawling.ipynb ì…€ 5\u001b[0m in \u001b[0;36m<cell line: 81>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sodud/study/ssafy_spec_pjt/subtitle/sub_file_crawling.ipynb#W5sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m \u001b[39m# ìë§‰ íŒŒì¼ í˜ì´ì§€ ëª©ë¡ íŒŒì¼\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sodud/study/ssafy_spec_pjt/subtitle/sub_file_crawling.ipynb#W5sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m page_list_file \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mC:/Users/sodud/study/ssafy_spec_pjt/subtitle/page_url_list.txt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/sodud/study/ssafy_spec_pjt/subtitle/sub_file_crawling.ipynb#W5sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m read_text_file(page_list_file, \u001b[39m0\u001b[39;49m, \u001b[39m10\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\sodud\\study\\ssafy_spec_pjt\\subtitle\\sub_file_crawling.ipynb ì…€ 5\u001b[0m in \u001b[0;36mread_text_file\u001b[1;34m(page_list_file, start, unit)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sodud/study/ssafy_spec_pjt/subtitle/sub_file_crawling.ipynb#W5sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m       \u001b[39mpass\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sodud/study/ssafy_spec_pjt/subtitle/sub_file_crawling.ipynb#W5sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m   \u001b[39mprint\u001b[39m(file_path)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/sodud/study/ssafy_spec_pjt/subtitle/sub_file_crawling.ipynb#W5sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m   file_search(file_urls)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sodud/study/ssafy_spec_pjt/subtitle/sub_file_crawling.ipynb#W5sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mif\u001b[39;00m flag \u001b[39m==\u001b[39m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sodud/study/ssafy_spec_pjt/subtitle/sub_file_crawling.ipynb#W5sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\sodud\\study\\ssafy_spec_pjt\\subtitle\\sub_file_crawling.ipynb ì…€ 5\u001b[0m in \u001b[0;36mfile_search\u001b[1;34m(file_urls)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sodud/study/ssafy_spec_pjt/subtitle/sub_file_crawling.ipynb#W5sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mì´ë¯¸ ì¡´ì¬í•˜ëŠ” íŒŒì¼: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m filename)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sodud/study/ssafy_spec_pjt/subtitle/sub_file_crawling.ipynb#W5sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/sodud/study/ssafy_spec_pjt/subtitle/sub_file_crawling.ipynb#W5sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     get_download(root_url \u001b[39m+\u001b[39;49m file_url\u001b[39m.\u001b[39;49mreplace(\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39m20\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mreplace(\u001b[39m\"\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39m27\u001b[39;49m\u001b[39m\"\u001b[39;49m), filename, path)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sodud/study/ssafy_spec_pjt/subtitle/sub_file_crawling.ipynb#W5sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sodud/study/ssafy_spec_pjt/subtitle/sub_file_crawling.ipynb#W5sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39më‹¤ìš´ë¡œë“œ ì¤‘ ì—ëŸ¬ ë°œìƒ: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m filename \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m err)\n",
      "\u001b[1;32mc:\\Users\\sodud\\study\\ssafy_spec_pjt\\subtitle\\sub_file_crawling.ipynb ì…€ 5\u001b[0m in \u001b[0;36mget_download\u001b[1;34m(url, fname, dir)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sodud/study/ssafy_spec_pjt/subtitle/sub_file_crawling.ipynb#W5sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39më‹¤ìš´ë¡œë“œ ì™„ë£Œ: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m fname)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sodud/study/ssafy_spec_pjt/subtitle/sub_file_crawling.ipynb#W5sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/sodud/study/ssafy_spec_pjt/subtitle/sub_file_crawling.ipynb#W5sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39;49m\u001b[39mì—ëŸ¬ ë°œìƒ:\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m err)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sodud/study/ssafy_spec_pjt/subtitle/sub_file_crawling.ipynb#W5sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate str (not \"HTTPError\") to str"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import os\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from inspect import getfile\n",
    "from urllib import request\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "def read_text_file(page_list_file, start, unit):\n",
    "  \n",
    "  flag = True\n",
    "  f = open(page_list_file, \"r\", encoding=\"UTF8\")\n",
    "  lines = f.readlines()[start: start+unit]\n",
    "  \n",
    "  if len(lines) == 0:\n",
    "    return\n",
    "  \n",
    "  if len(lines) < unit:\n",
    "    flag = False\n",
    "  \n",
    "  for line in lines:\n",
    "    if line[-3:] != \"%2F\":\n",
    "      pass\n",
    "    response = requests.get(line[:-1])\n",
    "    soup = bs(response.text, \"html.parser\")\n",
    "    files = soup.select(\"table#flisttable td > a\")\n",
    "    \n",
    "    file_urls = []\n",
    "    \n",
    "    # íŒŒì¼ ì½ê¸°\n",
    "    for f in files:\n",
    "      file_path= f.attrs[\"href\"]\n",
    "      \n",
    "      # srt íŒŒì¼ë§Œ ì €ì¥\n",
    "      if file_path[-3:] == \"srt\":\n",
    "        file_urls.append(\"/\" + file_path)\n",
    "      else:\n",
    "        pass\n",
    "    \n",
    "    print(file_path)\n",
    "    file_search(file_urls)\n",
    "    \n",
    "  if flag == False:\n",
    "    return\n",
    "  f.close\n",
    "  return read_text_file(page_list_file, start+unit, unit)\n",
    "\n",
    "# íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸\n",
    "def file_search(file_urls):\n",
    "  for file_url in file_urls:\n",
    "    # íŒŒì¼ ì´ë¦„ ì¶”ì¶œ\n",
    "    depth = file_url.split(\"/\")\n",
    "    filename = depth[len(depth)-1]\n",
    "    try:\n",
    "      path = \"C:\\\\Users\\\\sodud\\\\study\\\\ssafy_spec_pjt\\\\subtitle\\\\data\\\\\"\n",
    "      if os.path.isfile(path + filename):\n",
    "        print(\"ì´ë¯¸ ì¡´ì¬í•˜ëŠ” íŒŒì¼: \" + filename)\n",
    "      else:\n",
    "        get_download(root_url + file_url.replace(\" \", \"%20\").replace(\"'\", \"%27\"), filename, path)\n",
    "        \n",
    "    except HTTPError as err:\n",
    "      print(\"ë‹¤ìš´ë¡œë“œ ì¤‘ ì—ëŸ¬ ë°œìƒ: \" + filename + \"/\" + err)\n",
    "      pass\n",
    "\n",
    "# íŒŒì¼ ë‹¤ìš´ë¡œë“œ\n",
    "def get_download(url, fname, dir):\n",
    "  try:\n",
    "    os.chdir(dir)\n",
    "    request.urlretrieve(url, fname)\n",
    "    print(\"ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: \" + fname)\n",
    "  except HTTPError as err:\n",
    "    print(\"ì—ëŸ¬ ë°œìƒ:\" + err)\n",
    "    return\n",
    "\n",
    "# ìë§‰ ì‚¬ì´íŠ¸\n",
    "root_url = \"https://kitsunekko.net\"\n",
    "\n",
    "# ìë§‰ íŒŒì¼ í˜ì´ì§€ ëª©ë¡ íŒŒì¼\n",
    "page_list_file = \"C:/Users/sodud/study/ssafy_spec_pjt/subtitle/page_url_list.txt\"\n",
    "\n",
    "read_text_file(page_list_file, 0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœï¸ ìë§‰ì—ì„œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# ìë§‰ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ\n",
    "def get_subtitle(file_name):\n",
    "  print(\"# íŒŒì¼ëª…:\", file_name)\n",
    "  sub_text = []\n",
    "  \n",
    "  # íŒŒì¼ ì—´ê¸°\n",
    "  with open(path + file_name, \"rt\", encoding=\"UTF8\") as sub_file:\n",
    "    print(sub_file)\n",
    "    \n",
    "    # ì •ê·œì‹ ì •ì˜\n",
    "    regex = re.compile(r'\\d*\\n')\n",
    "    time = re.compile(r'(\\d{2}:\\d{2}:\\d{2})')\n",
    "    temp = re.compile(r\"'\\n'|' '\")\n",
    "    ch_list = [\"{\\\\an8}\", \"\\n\", \"\\u3000\", \"â™ªï½ï½â™ª\", \"1\", \"â™¬ï½\"]\n",
    "    \n",
    "    # íŒŒì¼ ë¼ì¸ë³„ë¡œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ\n",
    "    for line in sub_file:\n",
    "      if not (regex.match(line) or time.search(line) or temp.match(line) or len(line)==0):\n",
    "        line = line.strip()\n",
    "        for ch in ch_list:\n",
    "          line = line.replace(ch, \"\")\n",
    "        sub_text.append(line)\n",
    "    \n",
    "  with open(path + file_name, 'w', encoding=\"UTF8\") as f:\n",
    "    f.writelines(sub_text)\n",
    "    f.wirte(\"\\n\")\n",
    "  \n",
    "  return sub_text\n",
    "\n",
    "path = \"C:\\\\Users\\\\sodud\\\\study\\\\ssafy_spec_pjt\\\\subtitle\\\\data\\\\\"\n",
    "\n",
    "file_list = os.listdir(path)\n",
    "sub_files = [sub_file for sub_file in file_list]\n",
    "\n",
    "for sub_file in sub_files:\n",
    "  get_subtitle(sub_file)\n",
    "  print(sub_file + \" ìˆ˜ì • ì™„ë£Œ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
