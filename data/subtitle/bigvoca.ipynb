{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b1a6ad2",
   "metadata": {},
   "source": [
    "# âœï¸ í˜•íƒœì†Œ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259d1d8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "\n",
    "import MeCab\n",
    "import neologdn\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import googletrans\n",
    "\n",
    "\n",
    "# ë¶„ì„í•  ë‹¨ì–´ë§Œ ì¶”ì¶œ\n",
    "def is_content_word(feature):\n",
    "  \n",
    "  # í¬í•¨ í’ˆì‚¬: ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬, ë¶€ì‚¬\n",
    "  CONTENT_WORD_POS = (\"åè©\", \"å‹•è©\", \"å½¢å®¹è©\", \"å‰¯è©\")\n",
    "  # ì œì™¸ í’ˆì‚¬: ì ‘ë¯¸ì‚¬, ë¹„ìë¦½, ëŒ€ëª…ì‚¬, ë¯¸ì§€ì–´\n",
    "  IGNORE = (\"æ¥å°¾\", \"éè‡ªç«‹\", \"ä»£åè©\", \"æœªçŸ¥èª\")\n",
    "\n",
    "  return feature.startswith(CONTENT_WORD_POS) and all(f not in IGNORE for f in feature.split(\",\")[:7])\n",
    "\n",
    "\n",
    "# í˜•íƒœì†Œ ë¶„ì„\n",
    "def morphological_analysis(data):\n",
    "  \n",
    "  text = neologdn.normalize(data[2], repeat=2)\n",
    "  text = \"\".join([i for i in text if i.isalpha() or i.isspace()])\n",
    "  \n",
    "  # í˜•íƒœì†Œ ë¶„ì„\n",
    "  mecab = MeCab.Tagger(\"-O wakati -F%m\\\\t --unk-feature æœªçŸ¥èª\")\n",
    "  node = mecab.parseToNode(text)\n",
    "  \n",
    "  # í˜•íƒœì†Œ ë¶„ì„ ê²°ê³¼ ì €ì¥ ë¦¬ìŠ¤íŠ¸\n",
    "  # ë¶ˆìš©ì–´ ì„¤ì •\n",
    "  stop_words = stopwords.words(\"japanese\")\n",
    "  \n",
    "  word_list = []\n",
    "  \n",
    "  while node:\n",
    "    if is_content_word(node.feature):\n",
    "      \n",
    "      # ë‹¨ì–´ì¥ì— ë„£ì„ ë‹¨ì–´ ì„¤ì •\n",
    "      lemma = node.feature.split(\",\")[7] if len(node.feature.split(\",\")) > 7 and node.feature.split(\",\")[7] != \"*\" else node.surface\n",
    "            \n",
    "      # ë¶ˆìš©ì–´ ì œì™¸í•˜ê³  word_listì— ì €ì¥\n",
    "      if lemma not in stop_words:\n",
    "        word_list.append(lemma.split(\"-\")[0])\n",
    "      \n",
    "    node = node.next\n",
    "  \n",
    "  dbcol_subtitle.update_one({\"filename\": data[\"filename\"]}, {\"$set\": {\"words\": word_list}})\n",
    "\n",
    "    \n",
    "# ë¹ˆë„ìˆ˜ ê³„ì‚°  \n",
    "def word_frequency(data):\n",
    "  \n",
    "  count = Counter(data[\"words\"])\n",
    "  \n",
    "  # ë‹¨ì–´ì™€ ë¹ˆë„ìˆ˜ ë”•ì…”ë„ˆë¦¬ ì €ì¥ ë¦¬ìŠ¤íŠ¸\n",
    "  frequency = dict()\n",
    "  \n",
    "  for word, cnt in count.most_common():\n",
    "    frequency[word] = cnt\n",
    "  \n",
    "  dbcol_subtitle.update_one({\"filename\": data[\"filename\"]}, {\"$set\": {\"frequency\": frequency}})\n",
    "  \n",
    "# MongoDBì™€ ì—°ê²°\n",
    "conn = pymongo.MongoClient(\"localhost\", 27017)\n",
    "\n",
    "# DB ì„ íƒ\n",
    "db = conn.soaeng\n",
    "\n",
    "# Collection ì„ íƒ\n",
    "dbcol_subtitle = db.subtitle\n",
    "\n",
    "# subtitle ë°ì´í„°ë¥¼ í”„ë ˆì„ìœ¼ë¡œ ì„¤ì •\n",
    "df = pd.DataFrame(list(dbcol_subtitle.find({})))\n",
    "\n",
    "# í˜•íƒœì†Œ ë¶„ì„\n",
    "df.apply(morphological_analysis, axis = 1)\n",
    "\n",
    "# ë¹ˆë„ìˆ˜ ê³„ì‚°\n",
    "df.apply(word_frequency, axis=1)\n",
    "\n",
    "print(\"ì €ì¥ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42339252",
   "metadata": {},
   "source": [
    "# âœï¸ ë¹ˆë„ìˆ˜ í†µí•©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d6fcaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "\n",
    "def save_words(data):\n",
    "  word_list = data[\"frequency\"]\n",
    "  for word, cnt in word_list.items():\n",
    "    if dbcol_word.find_one({\"word\": word}) != None:\n",
    "      count = dbcol_word.find_one({\"word\": word})[\"frequency\"]\n",
    "    else:\n",
    "      count = 0\n",
    "    cnt += count\n",
    "    dbcol_word.update_one({\"word\": word}, {\"$set\": {\"frequency\": cnt}}, upsert = True)\n",
    "  dbcol_subtitle.update_one({\"filename\": data[\"filename\"]}, {\"$set\": {\"check\": 1}})\n",
    "\n",
    "# MongoDBì™€ ì—°ê²°\n",
    "conn = pymongo.MongoClient(\"localhost\", 27017)\n",
    "\n",
    "# DB ì„ íƒ\n",
    "db = conn.soaeng\n",
    "\n",
    "# Collection ì„ íƒ\n",
    "dbcol_subtitle = db.subtitle\n",
    "dbcol_word = db.word\n",
    "\n",
    "# subtitle ë°ì´í„°ë¥¼ í”„ë ˆì„ìœ¼ë¡œ ì„¤ì •\n",
    "df = pd.DataFrame(list(dbcol_subtitle.find( {\"$or\": [{\"check\": {\"$exists\": False}}, {\"check\": {\"$eq\": 0}}]})))\n",
    "\n",
    "print(df.apply(save_words, axis = 1))\n",
    "\n",
    "print(\"ë‹¨ì–´ ì €ì¥ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669f6f2b",
   "metadata": {},
   "source": [
    "# âœï¸ í•œêµ­ì–´ ë²ˆì—­\n",
    "\n",
    "### ğŸ”· googletrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1f54a6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë‹¨ì–´ ë²ˆì—­ ì™„ë£Œ - google\n"
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "def translation_using_googletrans(data):\n",
    "    \n",
    "  word = data[\"word\"]\n",
    "\n",
    "  trans = Translator()\n",
    "  trans.raise_Exception = True\n",
    "\n",
    "  # ì¼-í•œ ë‹¨ì–´ ë²ˆì—­\n",
    "  o = trans.translate(text=word, src = \"ja\", dest = \"ja\") # for ë°œìŒ í‘œê¸°\n",
    "  t = trans.translate(text=word, src = \"ja\", dest = \"ko\")\n",
    "  \n",
    "  pronunciation = o.pronunciation\n",
    "  korean = t.text\n",
    "\n",
    "  dbcol_word.update_one({\"word\": word}, {\"$set\": {\"korean\": korean, \"pronunciation\": pronunciation}})\n",
    "\n",
    "# MongoDBì™€ ì—°ê²°\n",
    "conn = pymongo.MongoClient(\"localhost\", 27017)\n",
    "\n",
    "# DB ì„ íƒ\n",
    "db = conn.soaeng\n",
    "\n",
    "# Collection ì„ íƒ\n",
    "dbcol_word = db.word\n",
    "\n",
    "# subtitle ë°ì´í„°ë¥¼ í”„ë ˆì„ìœ¼ë¡œ ì„¤ì •\n",
    "df = pd.DataFrame(list(dbcol_word.find( {\"$or\": [{\"korean\": {\"$exists\": False}}, {\"pronunciation\": {\"$exists\": False}}]}).sort(\"frequency\", -1).limit(1000)))\n",
    "# print(df)\n",
    "df.apply(translation_using_googletrans, axis = 1)\n",
    "\n",
    "print(\"ë‹¨ì–´ ë²ˆì—­ ì™„ë£Œ - google\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383eb05a",
   "metadata": {},
   "source": [
    "### ğŸ”· papago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e192ef36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë‹¨ì–´ ë²ˆì—­ ì™„ë£Œ - naver\n"
     ]
    }
   ],
   "source": [
    "import dotenv\n",
    "import os\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "dotenv.load_dotenv(dotenv.find_dotenv())\n",
    "client_id = os.environ[\"PAPAGO_ID\"] # ê°œë°œìì„¼í„°ì—ì„œ ë°œê¸‰ë°›ì€ Client ID ê°’\n",
    "client_secret = os.environ[\"PAPAGO_SECRET\"] # ê°œë°œìì„¼í„°ì—ì„œ ë°œê¸‰ë°›ì€ Client Secret ê°’\n",
    "\n",
    "url = \"https://openapi.naver.com/v1/papago/n2mt\"\n",
    "\n",
    "def translation_using_papago(data):\n",
    "    \n",
    "    word = data[\"word\"]\n",
    "    data = {\"text\" : word, \"source\" : \"ja\", \"target\": \"ko\"}\n",
    "#     data = \"source=\" + src + \"&target=\" +tgt+ \"&text=\" + word\n",
    "        \n",
    "    header = {\"X-Naver-Client-Id\":client_id, \"X-Naver-Client-Secret\":client_secret}\n",
    "\n",
    "    response = requests.post(url, headers=header, data= data)\n",
    "    rescode = response.status_code\n",
    "\n",
    "    if(rescode==200):\n",
    "        t_data = response.json()\n",
    "        result = response.json()[\"message\"][\"result\"][\"translatedText\"]\n",
    "        dbcol_word.update_one({\"word\": word}, {\"$set\": {\"papago\": result}})\n",
    "    else:\n",
    "        print(\"Error Code:\" , rescode)\n",
    "        return 0\n",
    "    \n",
    "# MongoDBì™€ ì—°ê²°\n",
    "conn = pymongo.MongoClient(\"localhost\", 27017)\n",
    "\n",
    "# DB ì„ íƒ\n",
    "db = conn.soaeng\n",
    "\n",
    "# Collection ì„ íƒ\n",
    "dbcol_word = db.word\n",
    "\n",
    "# subtitle ë°ì´í„°ë¥¼ í”„ë ˆì„ìœ¼ë¡œ ì„¤ì •\n",
    "df = pd.DataFrame(list(dbcol_word.find( {\"$or\": [{\"papago\": {\"$exists\": False}}]}).sort(\"frequency\", -1).limit(1000)))\n",
    "# print(df)\n",
    "df.apply(translation_using_papago, axis = 1)\n",
    "\n",
    "print(\"ë‹¨ì–´ ë²ˆì—­ ì™„ë£Œ - naver\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "eebaf1173d8d9c3c4ee9a7b8bb1432a7f576348d6cb7a26bc263375fbc310797"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
